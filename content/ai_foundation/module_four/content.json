{
  "module": "AI Foundation - Module Four",
  "title": "AI Ethics and Responsible AI",
  "description": "This module explores the ethical considerations, biases, fairness, transparency, and responsible development practices in artificial intelligence systems.",
  "duration": "4-6 weeks",
  "difficulty": "Intermediate",
  "prerequisites": "AI Foundation Modules One, Two, and Three, Understanding of AI and ML concepts",
  "learning_objectives": [
    "Understand the ethical implications of AI systems",
    "Learn about bias detection and mitigation strategies",
    "Master fairness and transparency in AI",
    "Understand responsible AI development practices",
    "Learn about AI governance and regulations",
    "Apply ethical principles to AI projects"
  ],
  "lessons": [
    {
      "lesson_id": "4.1",
      "title": "Introduction to AI Ethics",
      "duration": "75 minutes",
      "topics": [
        {
          "topic_id": "4.1.1",
          "title": "What is AI Ethics?",
          "content": "AI Ethics is the study of moral principles and values that should guide the development and use of artificial intelligence systems. It addresses questions about fairness, transparency, accountability, and the impact of AI on society.",
          "key_points": [
            "AI Ethics guides responsible AI development",
            "Addresses fairness, transparency, and accountability",
            "Considers societal impact of AI systems",
            "Ensures AI benefits humanity"
          ]
        },
        {
          "topic_id": "4.1.2",
          "title": "Key Ethical Principles",
          "content": "Several key principles guide ethical AI development: fairness, transparency, accountability, privacy, and beneficence. Understanding these principles is crucial for building responsible AI systems.",
          "key_points": [
            "Fairness: AI systems should treat all individuals equally",
            "Transparency: AI decisions should be explainable",
            "Accountability: Clear responsibility for AI outcomes",
            "Privacy: Protection of personal data and information"
          ]
        },
        {
          "topic_id": "4.1.3",
          "title": "Ethical Challenges in AI",
          "content": "AI systems present unique ethical challenges including algorithmic bias, privacy concerns, job displacement, and decision-making transparency. Understanding these challenges helps in developing better AI systems.",
          "key_points": [
            "Algorithmic bias can perpetuate existing inequalities",
            "Privacy concerns with data collection and usage",
            "Job displacement and economic impact",
            "Transparency in AI decision-making processes"
          ]
        }
      ],
      "activities": [
        {
          "type": "discussion",
          "title": "AI Ethics Case Studies",
          "description": "Analyze real-world AI ethics cases and their implications"
        },
        {
          "type": "debate",
          "title": "AI Ethics Debate",
          "description": "Debate ethical considerations in AI applications"
        }
      ]
    },
    {
      "lesson_id": "4.2",
      "title": "Bias in AI Systems",
      "duration": "90 minutes",
      "topics": [
        {
          "topic_id": "4.2.1",
          "title": "Understanding Algorithmic Bias",
          "content": "Algorithmic bias occurs when AI systems produce systematically prejudiced results due to erroneous assumptions in the machine learning process. This bias can perpetuate and amplify existing social inequalities.",
          "key_points": [
            "Bias can be introduced through training data",
            "Historical biases can be amplified by AI",
            "Bias can affect different demographic groups",
            "Bias detection requires careful analysis"
          ],
          "code_example": {
            "title": "Bias Detection and Analysis Example",
            "description": "Demonstrating how to detect and analyze bias in AI systems",
            "language": "python",
            "code": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\nclass BiasDetectionExample:\n    def __init__(self):\n        self.model = RandomForestClassifier(random_state=42)\n        self.is_trained = False\n    \n    def generate_biased_data(self):\n        \"\"\"Generate synthetic data with intentional bias\"\"\"\n        np.random.seed(42)\n        n_samples = 1000\n        \n        # Create biased dataset where certain groups have different outcomes\n        data = []\n        for i in range(n_samples):\n            # Simulate demographic groups (0: Group A, 1: Group B)\n            group = np.random.choice([0, 1], p=[0.7, 0.3])  # 70% Group A, 30% Group B\n            \n            # Features: age, income, education_level\n            age = np.random.normal(35 + group * 5, 10)  # Group B slightly older\n            income = np.random.normal(50000 + group * 20000, 15000)  # Group B higher income\n            education = np.random.normal(12 + group * 2, 2)  # Group B higher education\n            \n            # Biased outcome: Group B has higher approval rate\n            if group == 0:  # Group A\n                approval_prob = 0.3 + 0.4 * (income > 60000) + 0.2 * (education > 14)\n            else:  # Group B\n                approval_prob = 0.6 + 0.3 * (income > 60000) + 0.2 * (education > 14)\n            \n            approval = np.random.choice([0, 1], p=[1-approval_prob, approval_prob])\n            \n            data.append([age, income, education, group, approval])\n        \n        df = pd.DataFrame(data, columns=['age', 'income', 'education', 'group', 'approval'])\n        return df\n    \n    def train_model(self, df):\n        \"\"\"Train model on the data\"\"\"\n        X = df[['age', 'income', 'education']].values\n        y = df['approval'].values\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.model.fit(X_train, y_train)\n        self.is_trained = True\n        \n        # Overall accuracy\n        y_pred = self.model.predict(X_test)\n        overall_accuracy = accuracy_score(y_test, y_pred)\n        \n        return {\n            'overall_accuracy': overall_accuracy,\n            'X_test': X_test,\n            'y_test': y_test,\n            'y_pred': y_pred,\n            'df_test': df.iloc[-len(X_test):]\n        }\n    \n    def analyze_bias(self, results):\n        \"\"\"Analyze bias across different demographic groups\"\"\"\n        df_test = results['df_test'].copy()\n        df_test['predicted'] = results['y_pred']\n        \n        bias_analysis = {}\n        \n        for group in [0, 1]:\n            group_data = df_test[df_test['group'] == group]\n            \n            if len(group_data) > 0:\n                # Accuracy for this group\n                group_accuracy = accuracy_score(group_data['approval'], group_data['predicted'])\n                \n                # Approval rate for this group\n                actual_approval_rate = group_data['approval'].mean()\n                predicted_approval_rate = group_data['predicted'].mean()\n                \n                bias_analysis[f'group_{group}'] = {\n                    'accuracy': group_accuracy,\n                    'actual_approval_rate': actual_approval_rate,\n                    'predicted_approval_rate': predicted_approval_rate,\n                    'sample_size': len(group_data)\n                }\n        \n        return bias_analysis\n    \n    def calculate_fairness_metrics(self, bias_analysis):\n        \"\"\"Calculate fairness metrics\"\"\"\n        if len(bias_analysis) < 2:\n            return \"Not enough groups for comparison\"\n        \n        group_0 = bias_analysis['group_0']\n        group_1 = bias_analysis['group_1']\n        \n        # Demographic parity difference\n        demographic_parity_diff = abs(\n            group_1['predicted_approval_rate'] - group_0['predicted_approval_rate']\n        )\n        \n        # Equalized odds difference (simplified)\n        equalized_odds_diff = abs(\n            group_1['accuracy'] - group_0['accuracy']\n        )\n        \n        return {\n            'demographic_parity_difference': demographic_parity_diff,\n            'equalized_odds_difference': equalized_odds_diff,\n            'is_demographic_parity_fair': demographic_parity_diff < 0.1,\n            'is_equalized_odds_fair': equalized_odds_diff < 0.05\n        }\n\n# Example usage\nbias_example = BiasDetectionExample()\n\n# Generate biased data\nprint(\"Generating biased dataset...\")\ndf = bias_example.generate_biased_data()\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Approval rates by group:\")\nprint(df.groupby('group')['approval'].mean())\n\n# Train model\nprint(\"\\nTraining model...\")\nresults = bias_example.train_model(df)\nprint(f\"Overall accuracy: {results['overall_accuracy']:.3f}\")\n\n# Analyze bias\nprint(\"\\nAnalyzing bias...\")\nbias_analysis = bias_example.analyze_bias(results)\n\nfor group, metrics in bias_analysis.items()):\n    print(f\"\\n{group.upper()}:\")\n    print(f\"  Accuracy: {metrics['accuracy']:.3f}\")\n    print(f\"  Actual approval rate: {metrics['actual_approval_rate']:.3f}\")\n    print(f\"  Predicted approval rate: {metrics['predicted_approval_rate']:.3f}\")\n    print(f\"  Sample size: {metrics['sample_size']}\")\n\n# Calculate fairness metrics\nfairness_metrics = bias_example.calculate_fairness_metrics(bias_analysis)\nprint(f\"\\nFairness Metrics:\")\nprint(f\"Demographic parity difference: {fairness_metrics['demographic_parity_difference']:.3f}\")\nprint(f\"Equalized odds difference: {fairness_metrics['equalized_odds_difference']:.3f}\")\nprint(f\"Is demographic parity fair: {fairness_metrics['is_demographic_parity_fair']}\")\nprint(f\"Is equalized odds fair: {fairness_metrics['is_equalized_odds_fair']}\")"
          }
        },
        {
          "topic_id": "4.2.2",
          "title": "Types of Bias in AI",
          "content": "Different types of bias can affect AI systems: data bias, algorithmic bias, and user bias. Understanding these types helps in identifying and mitigating bias effectively.",
          "key_points": [
            "Data bias: Biases present in training data",
            "Algorithmic bias: Biases in the learning process",
            "User bias: Biases in how systems are used",
            "Intersectional bias: Multiple biases combined"
          ]
        },
        {
          "topic_id": "4.2.3",
          "title": "Bias Detection Methods",
          "content": "Various methods exist for detecting bias in AI systems including statistical parity, equalized odds, and demographic parity. These methods help identify unfair treatment across different groups.",
          "key_points": [
            "Statistical parity measures equal outcomes",
            "Equalized odds ensures equal true/false positive rates",
            "Demographic parity checks equal selection rates",
            "Bias detection requires diverse testing data"
          ]
        }
      ],
      "activities": [
        {
          "type": "hands_on",
          "title": "Bias Detection Exercise",
          "description": "Practice detecting bias in AI models using real datasets"
        },
        {
          "type": "analysis",
          "title": "Bias Analysis Project",
          "description": "Analyze bias in existing AI systems and propose solutions"
        }
      ]
    },
    {
      "lesson_id": "4.3",
      "title": "Fairness and Equity in AI",
      "duration": "85 minutes",
      "topics": [
        {
          "topic_id": "4.3.1",
          "title": "Fairness Metrics",
          "content": "Fairness metrics help quantify and measure fairness in AI systems. Common metrics include demographic parity, equalized odds, and individual fairness. Understanding these metrics is essential for building fair AI systems.",
          "key_points": [
            "Demographic parity measures equal selection rates",
            "Equalized odds ensures equal error rates",
            "Individual fairness treats similar cases similarly",
            "Multiple fairness metrics may conflict"
          ]
        },
        {
          "topic_id": "4.3.2",
          "title": "Fairness-Aware Machine Learning",
          "content": "Fairness-aware machine learning techniques aim to build models that are fair by design. These techniques include pre-processing, in-processing, and post-processing approaches to ensure fairness.",
          "key_points": [
            "Pre-processing: Clean biased training data",
            "In-processing: Modify learning algorithms",
            "Post-processing: Adjust model outputs",
            "Fairness constraints can be incorporated into training"
          ]
        },
        {
          "topic_id": "4.3.3",
          "title": "Equity in AI Applications",
          "content": "Ensuring equity in AI applications requires considering the impact on different demographic groups and ensuring that AI systems benefit all users equally. This includes healthcare, criminal justice, and financial applications.",
          "key_points": [
            "Healthcare AI must serve all patient populations",
            "Criminal justice AI must be fair across demographics",
            "Financial AI must not discriminate in lending",
            "Equity requires diverse development teams"
          ]
        }
      ],
      "activities": [
        {
          "type": "hands_on",
          "title": "Fairness Implementation",
          "description": "Implement fairness-aware algorithms in machine learning models"
        },
        {
          "type": "case_study",
          "title": "Fairness Case Studies",
          "description": "Analyze fairness issues in real AI applications"
        }
      ]
    },
    {
      "lesson_id": "4.4",
      "title": "Transparency and Explainability",
      "duration": "80 minutes",
      "topics": [
        {
          "topic_id": "4.4.1",
          "title": "The Need for Explainable AI",
          "content": "Explainable AI (XAI) refers to methods and techniques that make AI systems' decisions understandable to humans. This is crucial for building trust, ensuring accountability, and enabling human oversight.",
          "key_points": [
            "Explainable AI builds user trust",
            "Transparency enables accountability",
            "Human oversight requires understanding",
            "Regulatory compliance often requires explainability"
          ],
          "code_example": {
            "title": "Explainable AI Implementation Example",
            "description": "Demonstrating explainable AI techniques using SHAP and feature importance",
            "language": "python",
            "code": "import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nimport shap\n\nclass ExplainableAIExample:\n    def __init__(self):\n        self.model = RandomForestClassifier(n_estimators=100, random_state=42)\n        self.explainer = None\n        self.is_trained = False\n    \n    def generate_loan_data(self):\n        \"\"\"Generate sample loan application data\"\"\"\n        np.random.seed(42)\n        n_samples = 1000\n        \n        data = []\n        for i in range(n_samples):\n            age = np.random.normal(35, 10)\n            income = np.random.normal(60000, 20000)\n            credit_score = np.random.normal(700, 100)\n            debt_ratio = np.random.normal(0.3, 0.1)\n            employment_years = np.random.normal(5, 3)\n            \n            # Loan approval logic\n            approval_score = (\n                0.3 * (income > 50000) +\n                0.25 * (credit_score > 650) +\n                0.2 * (debt_ratio < 0.4) +\n                0.15 * (employment_years > 2) +\n                0.1 * (age > 25)\n            )\n            \n            approval = 1 if approval_score > 0.5 else 0\n            \n            data.append([age, income, credit_score, debt_ratio, employment_years, approval])\n        \n        df = pd.DataFrame(data, columns=['age', 'income', 'credit_score', 'debt_ratio', 'employment_years', 'approval'])\n        return df\n    \n    def train_model(self, df):\n        \"\"\"Train the model\"\"\"\n        X = df.drop('approval', axis=1)\n        y = df['approval']\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.model.fit(X_train, y_train)\n        self.is_trained = True\n        \n        # Create SHAP explainer\n        self.explainer = shap.TreeExplainer(self.model)\n        \n        return {\n            'X_train': X_train,\n            'X_test': X_test,\n            'y_train': y_train,\n            'y_test': y_test,\n            'feature_names': X.columns.tolist()\n        }\n    \n    def get_feature_importance(self):\n        \"\"\"Get feature importance from the model\"\"\"\n        if not self.is_trained:\n            return \"Model not trained yet\"\n        \n        importance = self.model.feature_importances_\n        feature_names = ['age', 'income', 'credit_score', 'debt_ratio', 'employment_years']\n        \n        feature_importance = dict(zip(feature_names, importance))\n        return dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True))\n    \n    def explain_prediction(self, sample_data):\n        \"\"\"Explain a specific prediction using SHAP\"\"\"\n        if not self.is_trained:\n            return \"Model not trained yet\"\n        \n        # Get SHAP values for the sample\n        shap_values = self.explainer.shap_values(sample_data)\n        \n        # Get feature contributions\n        feature_names = ['age', 'income', 'credit_score', 'debt_ratio', 'employment_years']\n        contributions = dict(zip(feature_names, shap_values[0]))\n        \n        return contributions\n    \n    def generate_explanation_report(self, sample_data, prediction):\n        \"\"\"Generate a human-readable explanation report\"\"\"\n        if not self.is_trained:\n            return \"Model not trained yet\"\n        \n        contributions = self.explain_prediction(sample_data)\n        \n        # Sort contributions by absolute value\n        sorted_contributions = sorted(contributions.items(), key=lambda x: abs(x[1]), reverse=True)\n        \n        report = f\"Loan Application Decision: {'APPROVED' if prediction == 1 else 'DENIED'}\\n\"\n        report += \"=\" * 50 + \"\\n\"\n        report += \"\\nKey factors influencing this decision:\\n\"\n        \n        for feature, contribution in sorted_contributions:\n            impact = \"positive\" if contribution > 0 else \"negative\"\n            report += f\"â€¢ {feature.replace('_', ' ').title()}: {impact} impact ({contribution:.3f})\\n\"\n        \n        return report\n    \n    def get_global_explanations(self, X_sample):\n        \"\"\"Get global explanations for the model\"\"\"\n        if not self.is_trained:\n            return \"Model not trained yet\"\n        \n        # Get SHAP values for sample of data\n        shap_values = self.explainer.shap_values(X_sample)\n        \n        # Calculate mean absolute SHAP values for global importance\n        mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n        feature_names = ['age', 'income', 'credit_score', 'debt_ratio', 'employment_years']\n        \n        global_importance = dict(zip(feature_names, mean_abs_shap))\n        return dict(sorted(global_importance.items(), key=lambda x: x[1], reverse=True))\n\n# Example usage\nxai_example = ExplainableAIExample()\n\n# Generate and train on data\nprint(\"Generating loan application data...\")\ndf = xai_example.generate_loan_data()\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Approval rate: {df['approval'].mean():.2%}\")\n\n# Train model\nprint(\"\\nTraining model...\")\nresults = xai_example.train_model(df)\nprint(\"Model trained successfully!\")\n\n# Get feature importance\nprint(\"\\nFeature Importance:\")\nimportance = xai_example.get_feature_importance()\nfor feature, imp in importance.items():\n    print(f\"{feature}: {imp:.3f}\")\n\n# Explain a specific prediction\nprint(\"\\nExplaining a specific prediction:\")\nsample = np.array([[30, 75000, 720, 0.25, 4]])  # Good applicant\nprediction = xai_example.model.predict(sample)[0]\nexplanation = xai_example.generate_explanation_report(sample, prediction)\nprint(explanation)\n\n# Get global explanations\nprint(\"\\nGlobal Feature Importance (SHAP):\")\nglobal_explanations = xai_example.get_global_explanations(results['X_test'][:100])\nfor feature, importance in global_explanations.items():\n    print(f\"{feature}: {importance:.3f}\")"
          }
        },
        {
          "topic_id": "4.4.2",
          "title": "Explainability Techniques",
          "content": "Various techniques exist for making AI systems explainable including feature importance, SHAP values, LIME, and decision trees. These techniques help users understand how AI systems make decisions.",
          "key_points": [
            "Feature importance shows which inputs matter most",
            "SHAP values provide detailed feature contributions",
            "LIME explains individual predictions locally",
            "Decision trees are inherently interpretable"
          ]
        },
        {
          "topic_id": "4.4.3",
          "title": "Interpretability vs Performance",
          "content": "There's often a trade-off between model interpretability and performance. Simple models are more interpretable but may have lower performance, while complex models may perform better but are harder to explain.",
          "key_points": [
            "Simple models are more interpretable",
            "Complex models may have better performance",
            "Trade-offs must be considered for each application",
            "Hybrid approaches can balance both needs"
          ]
        }
      ],
      "activities": [
        {
          "type": "hands_on",
          "title": "Explainability Tools",
          "description": "Use explainability tools to analyze model decisions"
        },
        {
          "type": "project",
          "title": "Explainable AI Project",
          "description": "Build an explainable AI system and document its decisions"
        }
      ]
    },
    {
      "lesson_id": "4.5",
      "title": "Privacy and Data Protection",
      "duration": "70 minutes",
      "topics": [
        {
          "topic_id": "4.5.1",
          "title": "Privacy in AI Systems",
          "content": "AI systems often require large amounts of personal data, raising privacy concerns. Understanding privacy implications and implementing privacy-preserving techniques is crucial for responsible AI development.",
          "key_points": [
            "AI systems often require personal data",
            "Privacy concerns affect user trust",
            "Data minimization reduces privacy risks",
            "Privacy by design should be implemented"
          ]
        },
        {
          "topic_id": "4.5.2",
          "title": "Privacy-Preserving Techniques",
          "content": "Various techniques exist for preserving privacy in AI systems including differential privacy, federated learning, and homomorphic encryption. These techniques help protect user data while maintaining model performance.",
          "key_points": [
            "Differential privacy adds noise to protect individuals",
            "Federated learning trains on distributed data",
            "Homomorphic encryption enables computation on encrypted data",
            "Privacy-preserving techniques may impact performance"
          ]
        },
        {
          "topic_id": "4.5.3",
          "title": "Data Protection Regulations",
          "content": "Various regulations govern data protection and privacy including GDPR, CCPA, and sector-specific laws. Understanding these regulations is essential for compliant AI development.",
          "key_points": [
            "GDPR protects EU citizens' data rights",
            "CCPA provides California privacy protections",
            "Sector-specific laws may apply",
            "Compliance requires ongoing monitoring"
          ]
        }
      ],
      "activities": [
        {
          "type": "hands_on",
          "title": "Privacy Implementation",
          "description": "Implement privacy-preserving techniques in AI models"
        },
        {
          "type": "analysis",
          "title": "Privacy Impact Assessment",
          "description": "Conduct privacy impact assessments for AI systems"
        }
      ]
    },
    {
      "lesson_id": "4.6",
      "title": "Responsible AI Development",
      "duration": "75 minutes",
      "topics": [
        {
          "topic_id": "4.6.1",
          "title": "AI Development Lifecycle",
          "content": "Responsible AI development requires integrating ethical considerations throughout the entire development lifecycle, from problem definition to deployment and monitoring.",
          "key_points": [
            "Ethics should be considered from the start",
            "Continuous monitoring is essential",
            "Stakeholder input should be sought",
            "Documentation should include ethical considerations"
          ]
        },
        {
          "topic_id": "4.6.2",
          "title": "AI Governance and Oversight",
          "content": "Effective AI governance requires clear policies, oversight mechanisms, and accountability structures. This includes ethical review boards, impact assessments, and ongoing monitoring.",
          "key_points": [
            "Governance frameworks guide AI development",
            "Oversight mechanisms ensure accountability",
            "Impact assessments identify potential harms",
            "Ongoing monitoring detects issues"
          ]
        },
        {
          "topic_id": "4.6.3",
          "title": "Best Practices for Responsible AI",
          "content": "Best practices for responsible AI development include diverse teams, comprehensive testing, stakeholder engagement, and continuous monitoring. These practices help ensure AI systems are beneficial and safe.",
          "key_points": [
            "Diverse teams reduce bias and improve outcomes",
            "Comprehensive testing identifies issues early",
            "Stakeholder engagement ensures broad perspectives",
            "Continuous monitoring detects emerging issues"
          ]
        }
      ],
      "activities": [
        {
          "type": "project",
          "title": "Responsible AI Framework",
          "description": "Develop a responsible AI framework for an organization"
        },
        {
          "type": "case_study",
          "title": "AI Governance Analysis",
          "description": "Analyze AI governance structures in different organizations"
        }
      ]
    }
  ],
  "assessments": [
    {
      "assessment_id": "1",
      "type": "quiz",
      "title": "AI Ethics and Responsible AI Quiz",
      "description": "Comprehensive quiz covering all module topics",
      "questions": [
        {
          "question": "What is the primary goal of AI Ethics?",
          "type": "multiple_choice",
          "options": [
            "To make AI systems faster",
            "To guide responsible AI development and use",
            "To reduce the cost of AI development",
            "To make AI systems more complex"
          ],
          "correct_answer": 1
        },
        {
          "question": "Which of the following is NOT a type of bias in AI systems?",
          "type": "multiple_choice",
          "options": [
            "Data bias",
            "Algorithmic bias",
            "User bias",
            "Performance bias"
          ],
          "correct_answer": 3
        },
        {
          "question": "What is the purpose of explainable AI (XAI)?",
          "type": "multiple_choice",
          "options": [
            "To make AI systems faster",
            "To make AI decisions understandable to humans",
            "To reduce the cost of AI development",
            "To make AI systems more accurate"
          ],
          "correct_answer": 1
        },
        {
          "question": "Which technique helps preserve privacy in AI systems?",
          "type": "multiple_choice",
          "options": [
            "Increasing data collection",
            "Differential privacy",
            "Reducing model complexity",
            "Using simpler algorithms"
          ],
          "correct_answer": 1
        },
        {
          "question": "What is a key principle of responsible AI development?",
          "type": "multiple_choice",
          "options": [
            "Maximizing profit",
            "Ensuring fairness and transparency",
            "Minimizing development time",
            "Reducing computational costs"
          ],
          "correct_answer": 1
        }
      ]
    },
    {
      "assessment_id": "2",
      "type": "project",
      "title": "Responsible AI Project",
      "description": "Develop a responsible AI system with ethical considerations",
      "requirements": [
        "Identify an AI application with ethical implications",
        "Conduct an ethical impact assessment",
        "Implement fairness and transparency measures",
        "Develop privacy-preserving techniques",
        "Create governance and monitoring frameworks",
        "Present findings and recommendations"
      ]
    }
  ],
  "resources": [
    {
      "type": "reading",
      "title": "Weapons of Math Destruction",
      "author": "Cathy O'Neil",
      "description": "Examination of how algorithms can perpetuate inequality and bias"
    },
    {
      "type": "reading",
      "title": "AI Superpowers",
      "author": "Kai-Fu Lee",
      "description": "Analysis of AI development and its societal implications"
    },
    {
      "type": "article",
      "title": "Fairness in Machine Learning",
      "source": "ACM",
      "description": "Comprehensive overview of fairness in machine learning systems"
    },
    {
      "type": "video",
      "title": "AI Ethics Course",
      "source": "MIT",
      "description": "Course on ethical considerations in artificial intelligence"
    },
    {
      "type": "interactive",
      "title": "AI Ethics Toolkit",
      "source": "Partnership on AI",
      "description": "Interactive tools for implementing ethical AI practices"
    }
  ],
  "next_module": "AI Foundation - Module Five: AI Applications and Industry"
} 