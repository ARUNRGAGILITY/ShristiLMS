{
  "module": "AI Foundation - Module Three",
  "title": "Deep Learning Fundamentals",
  "description": "This module explores the fundamentals of deep learning, including neural networks, convolutional networks, recurrent networks, and practical applications in modern AI systems.",
  "duration": "6-8 weeks",
  "difficulty": "Intermediate to Advanced",
  "prerequisites": "AI Foundation Module One and Two, Strong understanding of machine learning, Python programming, Linear algebra and calculus",
  "learning_objectives": [
    "Understand the fundamentals of neural networks and deep learning",
    "Learn about different types of neural network architectures",
    "Master the training process for deep learning models",
    "Understand convolutional neural networks for computer vision",
    "Learn about recurrent neural networks for sequential data",
    "Apply deep learning concepts to real-world problems"
  ],
  "lessons": [
    {
      "lesson_id": "3.1",
      "title": "Introduction to Neural Networks",
      "duration": "90 minutes",
      "topics": [
        {
          "topic_id": "3.1.1",
          "title": "What are Neural Networks?",
          "content": "Neural networks are computational models inspired by biological neural networks in the human brain. They consist of interconnected nodes (neurons) that process information and learn patterns from data. Neural networks form the foundation of deep learning and modern AI systems.",
          "key_points": [
            "Neural networks mimic the structure of biological brains",
            "They consist of layers of interconnected neurons",
            "Each neuron processes inputs and produces an output",
            "Networks learn by adjusting connection weights"
          ],
          "code_example": {
            "title": "Simple Neural Network Implementation",
            "description": "A basic implementation of a neural network from scratch",
            "language": "python",
            "code": "import numpy as np\n\nclass SimpleNeuralNetwork:\n    def __init__(self, input_size, hidden_size, output_size):\n        # Initialize weights and biases\n        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n        self.b1 = np.zeros((1, hidden_size))\n        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n        self.b2 = np.zeros((1, output_size))\n    \n    def sigmoid(self, x):\n        \"\"\"Activation function\"\"\"\n        return 1 / (1 + np.exp(-x))\n    \n    def sigmoid_derivative(self, x):\n        \"\"\"Derivative of sigmoid function\"\"\"\n        return x * (1 - x)\n    \n    def forward(self, X):\n        \"\"\"Forward propagation\"\"\"\n        # Hidden layer\n        self.z1 = np.dot(X, self.W1) + self.b1\n        self.a1 = self.sigmoid(self.z1)\n        \n        # Output layer\n        self.z2 = np.dot(self.a1, self.W2) + self.b2\n        self.a2 = self.sigmoid(self.z2)\n        \n        return self.a2\n    \n    def backward(self, X, y, learning_rate=0.1):\n        \"\"\"Backward propagation\"\"\"\n        m = X.shape[0]\n        \n        # Calculate gradients\n        dz2 = self.a2 - y\n        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n        \n        dz1 = np.dot(dz2, self.W2.T) * self.sigmoid_derivative(self.a1)\n        dW1 = (1/m) * np.dot(X.T, dz1)\n        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n        \n        # Update weights and biases\n        self.W2 -= learning_rate * dW2\n        self.b2 -= learning_rate * db2\n        self.W1 -= learning_rate * dW1\n        self.b1 -= learning_rate * db1\n    \n    def train(self, X, y, epochs=1000, learning_rate=0.1):\n        \"\"\"Train the neural network\"\"\"\n        losses = []\n        \n        for epoch in range(epochs):\n            # Forward pass\n            output = self.forward(X)\n            \n            # Calculate loss\n            loss = np.mean((output - y) ** 2)\n            losses.append(loss)\n            \n            # Backward pass\n            self.backward(X, y, learning_rate)\n            \n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n        \n        return losses\n    \n    def predict(self, X):\n        \"\"\"Make predictions\"\"\"\n        return self.forward(X)\n\n# Example usage: XOR problem\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([[0], [1], [1], [0]])\n\n# Create and train neural network\nnn = SimpleNeuralNetwork(input_size=2, hidden_size=4, output_size=1)\nlosses = nn.train(X, y, epochs=1000, learning_rate=0.1)\n\n# Test predictions\npredictions = nn.predict(X)\nprint(\"\\nXOR Predictions:\")\nfor i, (input_data, pred) in enumerate(zip(X, predictions)):\n    print(f\"Input: {input_data}, Predicted: {pred[0]:.3f}, Actual: {y[i][0]}\")"
          }
        },
        {
          "topic_id": "3.1.2",
          "title": "Basic Neural Network Architecture",
          "content": "A basic neural network consists of three main types of layers: input layer, hidden layers, and output layer. The input layer receives data, hidden layers process information, and the output layer produces predictions. Understanding this architecture is crucial for building effective models.",
          "key_points": [
            "Input layer: Receives the input data",
            "Hidden layers: Process and transform the data",
            "Output layer: Produces the final prediction",
            "Each layer contains multiple neurons (nodes)"
          ]
        },
        {
          "topic_id": "3.1.3",
          "title": "Activation Functions",
          "content": "Activation functions determine the output of each neuron and introduce non-linearity into the network. Common activation functions include ReLU, Sigmoid, and Tanh. The choice of activation function significantly impacts network performance and learning capability.",
          "key_points": [
            "ReLU: Most popular, helps with vanishing gradient problem",
            "Sigmoid: Outputs values between 0 and 1",
            "Tanh: Outputs values between -1 and 1",
            "Activation functions introduce non-linearity to the network"
          ]
        }
      ],
      "activities": [
        {
          "type": "hands_on",
          "title": "Build a Simple Neural Network",
          "description": "Create a basic neural network using Python and understand its components"
        },
        {
          "type": "visualization",
          "title": "Neural Network Visualization",
          "description": "Visualize how data flows through different network architectures"
        }
      ]
    },
    {
      "lesson_id": "3.2",
      "title": "Training Neural Networks",
      "duration": "120 minutes",
      "topics": [
        {
          "topic_id": "3.2.1",
          "title": "Forward Propagation",
          "content": "Forward propagation is the process of passing input data through the neural network to produce predictions. During this phase, each neuron computes its output based on weighted inputs and activation functions. Understanding forward propagation is essential for network operation.",
          "key_points": [
            "Data flows from input layer to output layer",
            "Each neuron computes weighted sum of inputs",
            "Activation function is applied to the weighted sum",
            "Output becomes input for the next layer"
          ]
        },
        {
          "topic_id": "3.2.2",
          "title": "Backpropagation and Gradient Descent",
          "content": "Backpropagation is the algorithm used to train neural networks by computing gradients of the loss function with respect to network weights. Gradient descent uses these gradients to update weights and minimize the loss function, enabling the network to learn from data.",
          "key_points": [
            "Backpropagation computes gradients efficiently",
            "Gradient descent minimizes the loss function",
            "Learning rate controls the size of weight updates",
            "The process iterates until convergence"
          ]
        },
        {
          "topic_id": "3.2.3",
          "title": "Loss Functions and Optimization",
          "content": "Loss functions measure how well the network predictions match the actual targets. Common loss functions include Mean Squared Error for regression and Cross-Entropy for classification. Optimization algorithms like Adam and SGD help minimize these loss functions effectively.",
          "key_points": [
            "Loss function measures prediction error",
            "Different loss functions for different problem types",
            "Optimizers control how weights are updated",
            "Adam optimizer is popular for its adaptive learning rates"
          ]
        }
      ],
      "activities": [
        {
          "type": "experiment",
          "title": "Training Process Visualization",
          "description": "Observe how loss decreases during training and weights update"
        },
        {
          "type": "hands_on",
          "title": "Implement Backpropagation",
          "description": "Manually implement backpropagation for a simple network"
        }
      ]
    },
    {
      "lesson_id": "3.3",
      "title": "Convolutional Neural Networks (CNNs)",
      "duration": "100 minutes",
      "topics": [
        {
          "topic_id": "3.3.1",
          "title": "Introduction to CNNs",
          "content": "Convolutional Neural Networks are specialized neural networks designed for processing grid-like data, particularly images. They use convolutional layers that apply filters to detect features like edges, textures, and patterns. CNNs have revolutionized computer vision and image recognition.",
          "key_points": [
            "CNNs are designed for grid-like data (images)",
            "Convolutional layers detect spatial features",
            "Pooling layers reduce spatial dimensions",
            "CNNs are translation invariant"
          ],
          "code_example": {
            "title": "Simple CNN Implementation",
            "description": "A basic implementation of a convolutional neural network using PyTorch",
            "language": "python",
            "code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(SimpleCNN, self).__init__()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        \n        # Pooling layer\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n        \n        # Dropout for regularization\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, x):\n        # First convolutional block\n        x = self.pool(F.relu(self.conv1(x)))  # 28x28 -> 14x14\n        \n        # Second convolutional block\n        x = self.pool(F.relu(self.conv2(x)))  # 14x14 -> 7x7\n        \n        # Flatten the output\n        x = x.view(-1, 32 * 7 * 7)\n        \n        # Fully connected layers\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x\n\nclass CNNExample:\n    def __init__(self):\n        self.model = SimpleCNN()\n        self.criterion = nn.CrossEntropyLoss()\n        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n    \n    def generate_sample_data(self):\n        \"\"\"Generate sample image data for demonstration\"\"\"\n        # Create random image data (batch_size, channels, height, width)\n        batch_size = 32\n        images = torch.randn(batch_size, 1, 28, 28)  # 28x28 grayscale images\n        labels = torch.randint(0, 10, (batch_size,))  # Random labels 0-9\n        \n        return images, labels\n    \n    def train_step(self, images, labels):\n        \"\"\"Single training step\"\"\"\n        self.optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = self.model(images)\n        loss = self.criterion(outputs, labels)\n        \n        # Backward pass\n        loss.backward()\n        self.optimizer.step()\n        \n        return loss.item()\n    \n    def predict(self, image):\n        \"\"\"Make prediction on a single image\"\"\"\n        self.model.eval()\n        with torch.no_grad():\n            output = self.model(image.unsqueeze(0))\n            _, predicted = torch.max(output, 1)\n            return predicted.item()\n    \n    def get_feature_maps(self, image):\n        \"\"\"Extract feature maps from convolutional layers\"\"\"\n        self.model.eval()\n        with torch.no_grad():\n            # Get activations from first conv layer\n            x = F.relu(self.model.conv1(image.unsqueeze(0)))\n            return x.squeeze(0)  # Remove batch dimension\n\n# Example usage\ncnn_example = CNNExample()\n\n# Generate sample data\nimages, labels = cnn_example.generate_sample_data()\nprint(f\"Sample data shape: {images.shape}\")\nprint(f\"Labels shape: {labels.shape}\")\n\n# Train for a few steps\nprint(\"\\nTraining the CNN:\")\nfor step in range(5):\n    loss = cnn_example.train_step(images, labels)\n    print(f\"Step {step + 1}, Loss: {loss:.4f}\")\n\n# Test prediction\nsample_image = torch.randn(1, 28, 28)\nprediction = cnn_example.predict(sample_image)\nprint(f\"\\nPrediction for sample image: Class {prediction}\")\n\n# Get feature maps\nfeature_maps = cnn_example.get_feature_maps(sample_image)\nprint(f\"\\nFeature maps shape: {feature_maps.shape}\")\nprint(f\"Number of feature maps: {feature_maps.shape[0]}\")"
          }
        },
        {
          "topic_id": "3.3.2",
          "title": "Convolutional Layers",
          "content": "Convolutional layers are the core components of CNNs that learn to detect features in input data. They use filters (kernels) that slide over the input to produce feature maps. These layers can detect edges, textures, and complex patterns depending on the learned filters.",
          "key_points": [
            "Filters slide over input to detect features",
            "Feature maps highlight detected patterns",
            "Multiple filters detect different features",
            "Stride controls how much the filter moves"
          ]
        },
        {
          "topic_id": "3.3.3",
          "title": "Pooling and Fully Connected Layers",
          "content": "Pooling layers reduce the spatial dimensions of feature maps while preserving important information. Max pooling and average pooling are common techniques. Fully connected layers at the end of CNNs combine features for final classification decisions.",
          "key_points": [
            "Pooling reduces spatial dimensions",
            "Max pooling selects maximum value in each region",
            "Fully connected layers combine features",
            "Dropout helps prevent overfitting"
          ]
        }
      ],
      "activities": [
        {
          "type": "hands_on",
          "title": "CNN for Image Classification",
          "description": "Build a CNN to classify images using popular datasets"
        },
        {
          "type": "visualization",
          "title": "Feature Map Visualization",
          "description": "Visualize what different layers of a CNN learn"
        }
      ]
    },
    {
      "lesson_id": "3.4",
      "title": "Recurrent Neural Networks (RNNs)",
      "duration": "90 minutes",
      "topics": [
        {
          "topic_id": "3.4.1",
          "title": "Understanding RNNs",
          "content": "Recurrent Neural Networks are designed to process sequential data by maintaining internal memory. Unlike feedforward networks, RNNs have connections that form cycles, allowing them to remember previous inputs. This makes them ideal for tasks involving time series, text, and speech.",
          "key_points": [
            "RNNs process sequential data",
            "They maintain internal memory (hidden state)",
            "Same weights are applied at each time step",
            "Can handle variable-length sequences"
          ]
        },
        {
          "topic_id": "3.4.2",
          "title": "Long Short-Term Memory (LSTM)",
          "content": "LSTM is a specialized RNN architecture designed to address the vanishing gradient problem in traditional RNNs. It uses gates (input, forget, output) to control information flow and maintain long-term dependencies. LSTMs are widely used in natural language processing and speech recognition.",
          "key_points": [
            "LSTM addresses vanishing gradient problem",
            "Uses gates to control information flow",
            "Can maintain long-term dependencies",
            "Widely used in NLP and speech recognition"
          ]
        },
        {
          "topic_id": "3.4.3",
          "title": "Gated Recurrent Units (GRU)",
          "content": "GRU is a simplified version of LSTM that uses fewer parameters while maintaining similar performance. It combines the forget and input gates into a single update gate and merges the cell state and hidden state. GRUs are computationally efficient and often preferred for simpler tasks.",
          "key_points": [
            "GRU is simpler than LSTM",
            "Uses fewer parameters",
            "Similar performance to LSTM",
            "Computationally more efficient"
          ]
        }
      ],
      "activities": [
        {
          "type": "hands_on",
          "title": "Text Generation with RNN",
          "description": "Build an RNN to generate text sequences"
        },
        {
          "type": "project",
          "title": "Sentiment Analysis",
          "description": "Use RNNs for sentiment analysis of text data"
        }
      ]
    },
    {
      "lesson_id": "3.5",
      "title": "Training Deep Learning Models",
      "duration": "110 minutes",
      "topics": [
        {
          "topic_id": "3.5.1",
          "title": "Data Preparation for Deep Learning",
          "content": "Deep learning models require large amounts of high-quality data. Data preparation includes normalization, augmentation, and proper splitting into training, validation, and test sets. The quality of data preparation significantly impacts model performance.",
          "key_points": [
            "Normalize data to similar scales",
            "Data augmentation increases dataset size",
            "Proper train/validation/test split is crucial",
            "Quality data preparation improves performance"
          ]
        },
        {
          "topic_id": "3.5.2",
          "title": "Regularization Techniques",
          "content": "Regularization techniques prevent overfitting in deep learning models. Common techniques include dropout, weight decay, early stopping, and data augmentation. These methods help models generalize better to unseen data.",
          "key_points": [
            "Dropout randomly deactivates neurons during training",
            "Weight decay penalizes large weights",
            "Early stopping prevents overfitting",
            "Regularization improves generalization"
          ]
        },
        {
          "topic_id": "3.5.3",
          "title": "Hyperparameter Tuning",
          "content": "Hyperparameter tuning is crucial for optimizing deep learning model performance. Important hyperparameters include learning rate, batch size, number of layers, and number of neurons. Systematic tuning using techniques like grid search or Bayesian optimization improves model performance.",
          "key_points": [
            "Learning rate affects training speed and convergence",
            "Batch size impacts memory usage and training stability",
            "Network architecture affects model capacity",
            "Systematic tuning improves performance"
          ]
        }
      ],
      "activities": [
        {
          "type": "experiment",
          "title": "Hyperparameter Tuning",
          "description": "Experiment with different hyperparameters and observe effects"
        },
        {
          "type": "analysis",
          "title": "Model Performance Analysis",
          "description": "Analyze training curves and model performance metrics"
        }
      ]
    },
    {
      "lesson_id": "3.6",
      "title": "Transfer Learning and Pre-trained Models",
      "duration": "80 minutes",
      "topics": [
        {
          "topic_id": "3.6.1",
          "title": "Understanding Transfer Learning",
          "content": "Transfer learning involves using knowledge learned from one task to improve performance on a related task. Instead of training models from scratch, transfer learning leverages pre-trained models and adapts them to new problems. This approach is especially valuable when limited data is available.",
          "key_points": [
            "Transfer learning reuses learned features",
            "Reduces training time and data requirements",
            "Improves performance on related tasks",
            "Especially useful for limited data scenarios"
          ]
        },
        {
          "topic_id": "3.6.2",
          "title": "Popular Pre-trained Models",
          "content": "Several pre-trained models are widely used in deep learning applications. For computer vision, models like VGG, ResNet, and EfficientNet are popular. For natural language processing, BERT, GPT, and Transformer models are commonly used. These models provide strong baselines for various tasks.",
          "key_points": [
            "ResNet: Deep residual networks for image classification",
            "BERT: Bidirectional transformers for language understanding",
            "GPT: Generative pre-trained transformers for text generation",
            "EfficientNet: Optimized CNN architecture"
          ]
        },
        {
          "topic_id": "3.6.3",
          "title": "Fine-tuning Strategies",
          "content": "Fine-tuning involves adapting pre-trained models to specific tasks. Strategies include freezing early layers and training only later layers, or using different learning rates for different layers. Proper fine-tuning can significantly improve performance while reducing training time.",
          "key_points": [
            "Freeze early layers, train later layers",
            "Use different learning rates for different layers",
            "Gradually unfreeze layers during training",
            "Fine-tuning requires less data than training from scratch"
          ]
        }
      ],
      "activities": [
        {
          "type": "hands_on",
          "title": "Transfer Learning Project",
          "description": "Use pre-trained models for a custom classification task"
        },
        {
          "type": "case_study",
          "title": "Pre-trained Model Analysis",
          "description": "Analyze and compare different pre-trained models"
        }
      ]
    },
    {
      "lesson_id": "3.7",
      "title": "Deep Learning Applications",
      "duration": "70 minutes",
      "topics": [
        {
          "topic_id": "3.7.1",
          "title": "Computer Vision Applications",
          "content": "Deep learning has revolutionized computer vision with applications in image classification, object detection, semantic segmentation, and facial recognition. CNNs are the primary architecture for these tasks, achieving human-level or better performance in many cases.",
          "key_points": [
            "Image classification: Categorizing images",
            "Object detection: Locating objects in images",
            "Semantic segmentation: Pixel-level classification",
            "Facial recognition: Identifying individuals"
          ]
        },
        {
          "topic_id": "3.7.2",
          "title": "Natural Language Processing",
          "content": "Deep learning has transformed natural language processing with applications in machine translation, text generation, sentiment analysis, and question answering. Transformer-based models like BERT and GPT have achieved remarkable performance on various NLP tasks.",
          "key_points": [
            "Machine translation: Converting between languages",
            "Text generation: Creating human-like text",
            "Sentiment analysis: Understanding text emotions",
            "Question answering: Extracting answers from text"
          ]
        },
        {
          "topic_id": "3.7.3",
          "title": "Emerging Applications",
          "content": "Deep learning is expanding into new domains including healthcare, autonomous vehicles, robotics, and creative applications. These applications demonstrate the versatility and potential of deep learning to solve complex real-world problems.",
          "key_points": [
            "Healthcare: Medical imaging and drug discovery",
            "Autonomous vehicles: Perception and decision making",
            "Robotics: Control and manipulation",
            "Creative AI: Art, music, and content generation"
          ]
        }
      ],
      "activities": [
        {
          "type": "case_study",
          "title": "Deep Learning Application Analysis",
          "description": "Analyze a real-world deep learning application"
        },
        {
          "type": "discussion",
          "title": "Future of Deep Learning",
          "description": "Discuss emerging trends and future applications"
        }
      ]
    }
  ],
  "assessments": [
    {
      "assessment_id": "1",
      "type": "quiz",
      "title": "Deep Learning Fundamentals Quiz",
      "description": "Comprehensive quiz covering all module topics",
      "questions": [
        {
          "question": "What is the main advantage of using activation functions in neural networks?",
          "type": "multiple_choice",
          "options": [
            "They make the network faster",
            "They introduce non-linearity to the network",
            "They reduce the number of parameters",
            "They make the network more accurate"
          ],
          "correct_answer": 1
        },
        {
          "question": "Which neural network architecture is best suited for processing images?",
          "type": "multiple_choice",
          "options": [
            "Recurrent Neural Networks (RNN)",
            "Convolutional Neural Networks (CNN)",
            "Long Short-Term Memory (LSTM)",
            "Gated Recurrent Units (GRU)"
          ],
          "correct_answer": 1
        },
        {
          "question": "What is the purpose of backpropagation in neural networks?",
          "type": "multiple_choice",
          "options": [
            "To forward data through the network",
            "To compute gradients for updating weights",
            "To initialize network weights",
            "To evaluate model performance"
          ],
          "correct_answer": 1
        },
        {
          "question": "Which technique helps prevent overfitting in deep learning models?",
          "type": "multiple_choice",
          "options": [
            "Increasing the learning rate",
            "Using dropout during training",
            "Reducing the dataset size",
            "Using more layers"
          ],
          "correct_answer": 1
        },
        {
          "question": "What is transfer learning in deep learning?",
          "type": "multiple_choice",
          "options": [
            "Training models from scratch",
            "Using knowledge from one task to improve another",
            "Transferring data between different datasets",
            "Moving models between different hardware"
          ],
          "correct_answer": 1
        }
      ]
    },
    {
      "assessment_id": "2",
      "type": "project",
      "title": "Deep Learning Project",
      "description": "Complete a deep learning project using neural networks",
      "requirements": [
        "Choose a dataset and define the problem",
        "Design and implement a neural network architecture",
        "Train the model with proper hyperparameter tuning",
        "Evaluate model performance using appropriate metrics",
        "Apply transfer learning techniques if applicable",
        "Present findings and demonstrate the model"
      ]
    }
  ],
  "resources": [
    {
      "type": "reading",
      "title": "Deep Learning",
      "author": "Ian Goodfellow, Yoshua Bengio, Aaron Courville",
      "description": "Comprehensive textbook covering deep learning fundamentals and advanced topics"
    },
    {
      "type": "video",
      "title": "Deep Learning Specialization",
      "source": "Coursera - Andrew Ng",
      "description": "Comprehensive course series on deep learning fundamentals and applications"
    },
    {
      "type": "article",
      "title": "Understanding Convolutional Neural Networks",
      "source": "Towards Data Science",
      "description": "Detailed explanation of CNN architecture and applications"
    },
    {
      "type": "interactive",
      "title": "Neural Network Playground",
      "source": "TensorFlow",
      "description": "Interactive visualization of neural networks and training process"
    },
    {
      "type": "reading",
      "title": "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow",
      "author": "Aurélien Géron",
      "description": "Practical guide to implementing deep learning models with popular frameworks"
    }
  ],
  "next_module": "Course Complete - Congratulations!"
} 