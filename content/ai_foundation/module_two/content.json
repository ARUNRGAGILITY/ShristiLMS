{
  "module": "AI Foundation - Module Two",
  "title": "Machine Learning Fundamentals",
  "description": "This module covers the fundamentals of machine learning, including algorithms, data preprocessing, model training, and evaluation techniques.",
  "duration": "5-7 weeks",
  "difficulty": "Beginner to Intermediate",
  "prerequisites": "AI Foundation Module One, Basic Python programming, High school mathematics",
  "learning_objectives": [
    "Understand the core concepts and types of machine learning",
    "Learn about supervised, unsupervised, and reinforcement learning",
    "Master data preprocessing and feature engineering techniques",
    "Understand model training, validation, and evaluation",
    "Learn about common machine learning algorithms",
    "Apply machine learning concepts to real-world problems"
  ],
  "lessons": [
    {
      "lesson_id": "2.1",
      "title": "Introduction to Machine Learning",
      "duration": "60 minutes",
      "topics": [
        {
          "topic_id": "2.1.1",
          "title": "What is Machine Learning?",
          "content": "Machine Learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can access data and use it to learn for themselves.",
          "key_points": [
            "ML enables computers to learn from data without explicit programming",
            "The goal is to find patterns in data and make predictions",
            "ML algorithms improve their performance over time",
            "ML is the foundation for many AI applications"
          ],
          "code_example": {
            "title": "Simple Machine Learning Example",
            "description": "A basic example of how machine learning can learn patterns from data",
            "language": "python",
            "code": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Simple ML example: Predicting house prices based on size\nclass SimpleMLExample:\n    def __init__(self):\n        self.model = LinearRegression()\n        self.is_trained = False\n    \n    def generate_sample_data(self):\n        # Generate sample house data (size in sq ft, price in thousands)\n        np.random.seed(42)\n        house_sizes = np.random.randint(800, 3000, 100)\n        # Price = size * 0.2 + random noise\n        house_prices = house_sizes * 0.2 + np.random.normal(0, 50, 100)\n        return house_sizes.reshape(-1, 1), house_prices\n    \n    def train_model(self, X, y):\n        # Split data into training and testing sets\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n        \n        # Train the model\n        self.model.fit(X_train, y_train)\n        self.is_trained = True\n        \n        # Evaluate the model\n        train_score = self.model.score(X_train, y_train)\n        test_score = self.model.score(X_test, y_test)\n        \n        return {\n            'train_score': train_score,\n            'test_score': test_score,\n            'coefficient': self.model.coef_[0],\n            'intercept': self.model.intercept_\n        }\n    \n    def predict_price(self, house_size):\n        if not self.is_trained:\n            return \"Model not trained yet\"\n        prediction = self.model.predict([[house_size]])[0]\n        return f\"Predicted price for {house_size} sq ft: ${prediction:.2f}k\"\n\n# Example usage\nml_example = SimpleMLExample()\nX, y = ml_example.generate_sample_data()\nresults = ml_example.train_model(X, y)\nprint(f\"Training R² score: {results['train_score']:.3f}\")\nprint(f\"Testing R² score: {results['test_score']:.3f}\")\nprint(f\"Price per sq ft: ${results['coefficient']:.3f}k\")\nprint(ml_example.predict_price(2000))"
          }
        },
        {
          "topic_id": "2.1.2",
          "title": "Types of Machine Learning",
          "content": "Machine learning can be categorized into three main types: Supervised Learning, Unsupervised Learning, and Reinforcement Learning. Each type has different approaches and applications.",
          "key_points": [
            "Supervised Learning: Learning from labeled training data",
            "Unsupervised Learning: Finding patterns in unlabeled data",
            "Reinforcement Learning: Learning through trial and error",
            "Each type serves different purposes and applications"
          ]
        },
        {
          "topic_id": "2.1.3",
          "title": "The Machine Learning Process",
          "content": "The machine learning process involves several key steps: data collection, preprocessing, feature engineering, model selection, training, evaluation, and deployment. Understanding this workflow is crucial for successful ML projects.",
          "key_points": [
            "Data collection and preparation is the foundation",
            "Feature engineering transforms raw data into useful features",
            "Model selection depends on the problem type and data",
            "Evaluation ensures model performance and reliability"
          ]
        }
      ],
      "activities": [
        {
          "type": "discussion",
          "title": "ML Applications in Daily Life",
          "description": "Identify and discuss machine learning applications you encounter daily"
        },
        {
          "type": "quiz",
          "title": "Machine Learning Basics Quiz",
          "description": "Test understanding of ML concepts and types"
        }
      ]
    },
    {
      "lesson_id": "2.2",
      "title": "Supervised Learning",
      "duration": "90 minutes",
      "topics": [
        {
          "topic_id": "2.2.1",
          "title": "Understanding Supervised Learning",
          "content": "Supervised learning involves training a model on labeled data, where the correct answers are provided. The model learns to map inputs to outputs and can then make predictions on new, unseen data.",
          "key_points": [
            "Uses labeled training data with known outcomes",
            "Goal is to learn the mapping from inputs to outputs",
            "Can be used for classification and regression problems",
            "Requires high-quality labeled data for training"
          ],
          "code_example": {
            "title": "Supervised Learning Classification Example",
            "description": "Demonstrating supervised learning with a simple classification problem",
            "language": "python",
            "code": "import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\nclass SupervisedLearningExample:\n    def __init__(self):\n        self.model = RandomForestClassifier(n_estimators=100, random_state=42)\n        self.is_trained = False\n    \n    def generate_classification_data(self):\n        # Generate synthetic classification data\n        # Features: [age, income, credit_score]\n        # Target: 0 (no loan approval), 1 (loan approval)\n        X, y = make_classification(\n            n_samples=1000,\n            n_features=3,\n            n_informative=3,\n            n_redundant=0,\n            n_classes=2,\n            random_state=42\n        )\n        \n        # Add meaningful feature names\n        feature_names = ['age', 'income', 'credit_score']\n        return X, y, feature_names\n    \n    def train_model(self, X, y):\n        # Split data into training and testing sets\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=42, stratify=y\n        )\n        \n        # Train the model\n        self.model.fit(X_train, y_train)\n        self.is_trained = True\n        \n        # Make predictions\n        y_pred = self.model.predict(X_test)\n        \n        # Calculate accuracy\n        accuracy = accuracy_score(y_test, y_pred)\n        \n        return {\n            'accuracy': accuracy,\n            'classification_report': classification_report(y_test, y_pred),\n            'feature_importance': self.model.feature_importances_\n        }\n    \n    def predict_loan_approval(self, age, income, credit_score):\n        if not self.is_trained:\n            return \"Model not trained yet\"\n        \n        features = np.array([[age, income, credit_score]])\n        prediction = self.model.predict(features)[0]\n        probability = self.model.predict_proba(features)[0]\n        \n        result = \"APPROVED\" if prediction == 1 else \"DENIED\"\n        confidence = probability[1] if prediction == 1 else probability[0]\n        \n        return f\"Loan {result} (Confidence: {confidence:.2%})\"\n\n# Example usage\nsl_example = SupervisedLearningExample()\nX, y, feature_names = sl_example.generate_classification_data()\nresults = sl_example.train_model(X, y)\n\nprint(f\"Model Accuracy: {results['accuracy']:.3f}\")\nprint(\"\\nClassification Report:\")\nprint(results['classification_report'])\nprint(\"\\nFeature Importance:\")\nfor name, importance in zip(feature_names, results['feature_importance']):\n    print(f\"{name}: {importance:.3f}\")\n\n# Test predictions\nprint(\"\\nSample Predictions:\")\nprint(sl_example.predict_loan_approval(25, 50000, 750))\nprint(sl_example.predict_loan_approval(45, 80000, 650))\nprint(sl_example.predict_loan_approval(30, 120000, 850))"
          }
        },
        {
          "topic_id": "2.2.2",
          "title": "Classification vs Regression",
          "content": "Supervised learning can be divided into two main categories: Classification (predicting categories) and Regression (predicting continuous values). Understanding the difference is crucial for choosing the right approach.",
          "key_points": [
            "Classification: Predicts discrete categories or classes",
            "Regression: Predicts continuous numerical values",
            "Classification examples: spam detection, image recognition",
            "Regression examples: house prices, temperature prediction"
          ]
        },
        {
          "topic_id": "2.2.3",
          "title": "Common Supervised Learning Algorithms",
          "content": "Several algorithms are commonly used in supervised learning, each with its strengths and weaknesses. Understanding these algorithms helps in choosing the right one for specific problems.",
          "key_points": [
            "Linear Regression: For continuous value prediction",
            "Logistic Regression: For binary classification",
            "Decision Trees: Easy to understand and interpret",
            "Random Forest: Ensemble method for better performance",
            "Support Vector Machines: Effective for classification",
            "Neural Networks: Powerful but complex models"
          ]
        }
      ],
      "activities": [
        {
          "type": "hands_on",
          "title": "Simple Classification Project",
          "description": "Build a basic classification model using a dataset"
        },
        {
          "type": "case_study",
          "title": "Supervised Learning Applications",
          "description": "Analyze real-world supervised learning applications"
        }
      ]
    },
    {
      "lesson_id": "2.3",
      "title": "Unsupervised Learning",
      "duration": "75 minutes",
      "topics": [
        {
          "topic_id": "2.3.1",
          "title": "Understanding Unsupervised Learning",
          "content": "Unsupervised learning works with unlabeled data to find hidden patterns and structures. Unlike supervised learning, there are no correct answers provided during training.",
          "key_points": [
            "Works with unlabeled data without known outcomes",
            "Goal is to discover hidden patterns and structures",
            "Used for clustering, dimensionality reduction, and association",
            "Valuable for exploratory data analysis"
          ],
          "code_example": {
            "title": "Unsupervised Learning Clustering Example",
            "description": "Demonstrating unsupervised learning with K-means clustering",
            "language": "python",
            "code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import silhouette_score\n\nclass UnsupervisedLearningExample:\n    def __init__(self):\n        self.kmeans = None\n        self.is_trained = False\n    \n    def generate_clustering_data(self):\n        # Generate synthetic data with 3 clusters\n        X, true_labels = make_blobs(\n            n_samples=300,\n            centers=3,\n            cluster_std=0.60,\n            random_state=42\n        )\n        return X, true_labels\n    \n    def find_optimal_clusters(self, X, max_clusters=10):\n        \"\"\"Find optimal number of clusters using silhouette score\"\"\"\n        silhouette_scores = []\n        K_range = range(2, max_clusters + 1)\n        \n        for k in K_range:\n            kmeans = KMeans(n_clusters=k, random_state=42)\n            cluster_labels = kmeans.fit_predict(X)\n            silhouette_avg = silhouette_score(X, cluster_labels)\n            silhouette_scores.append(silhouette_avg)\n        \n        optimal_k = K_range[np.argmax(silhouette_scores)]\n        return optimal_k, silhouette_scores\n    \n    def train_clustering_model(self, X, n_clusters=3):\n        # Train K-means clustering model\n        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n        cluster_labels = self.kmeans.fit_predict(X)\n        self.is_trained = True\n        \n        # Calculate silhouette score\n        silhouette_avg = silhouette_score(X, cluster_labels)\n        \n        return {\n            'cluster_labels': cluster_labels,\n            'cluster_centers': self.kmeans.cluster_centers_,\n            'silhouette_score': silhouette_avg,\n            'inertia': self.kmeans.inertia_\n        }\n    \n    def predict_cluster(self, new_data):\n        if not self.is_trained:\n            return \"Model not trained yet\"\n        \n        cluster = self.kmeans.predict([new_data])[0]\n        distance_to_center = np.linalg.norm(new_data - self.kmeans.cluster_centers_[cluster])\n        \n        return f\"Assigned to cluster {cluster} (Distance to center: {distance_to_center:.3f})\"\n    \n    def visualize_clusters(self, X, cluster_labels, centers):\n        \"\"\"Visualize the clustering results\"\"\"\n        plt.figure(figsize=(10, 6))\n        \n        # Plot data points\n        scatter = plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis')\n        \n        # Plot cluster centers\n        plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='x', s=200, linewidths=3)\n        \n        plt.title('K-means Clustering Results')\n        plt.xlabel('Feature 1')\n        plt.ylabel('Feature 2')\n        plt.colorbar(scatter)\n        plt.show()\n\n# Example usage\nul_example = UnsupervisedLearningExample()\nX, true_labels = ul_example.generate_clustering_data()\n\n# Find optimal number of clusters\noptimal_k, scores = ul_example.find_optimal_clusters(X)\nprint(f\"Optimal number of clusters: {optimal_k}\")\nprint(f\"Silhouette scores: {[f'{s:.3f}' for s in scores]}\")\n\n# Train clustering model\nresults = ul_example.train_clustering_model(X, optimal_k)\nprint(f\"\\nClustering Results:\")\nprint(f\"Silhouette Score: {results['silhouette_score']:.3f}\")\nprint(f\"Inertia: {results['inertia']:.3f}\")\nprint(f\"Number of clusters: {len(results['cluster_centers'])}\")\n\n# Test predictions\nprint(\"\\nSample Predictions:\")\nprint(ul_example.predict_cluster([2, 2]))\nprint(ul_example.predict_cluster([-2, -2]))\nprint(ul_example.predict_cluster([0, 0]))\n\n# Visualize results (uncomment to see plot)\n# ul_example.visualize_clusters(X, results['cluster_labels'], results['cluster_centers'])"
          }
        },
        {
          "topic_id": "2.3.2",
          "title": "Clustering Algorithms",
          "content": "Clustering is a common unsupervised learning technique that groups similar data points together. Popular algorithms include K-means, hierarchical clustering, and DBSCAN.",
          "key_points": [
            "K-means: Groups data into K clusters based on similarity",
            "Hierarchical Clustering: Creates a tree of clusters",
            "DBSCAN: Density-based clustering for irregular shapes",
            "Clustering is useful for customer segmentation and data exploration"
          ]
        },
        {
          "topic_id": "2.3.3",
          "title": "Dimensionality Reduction",
          "content": "Dimensionality reduction techniques help reduce the number of features while preserving important information. This is crucial for handling high-dimensional data and improving model performance.",
          "key_points": [
            "Principal Component Analysis (PCA): Most common technique",
            "t-SNE: Good for visualization of high-dimensional data",
            "Reduces computational complexity and overfitting",
            "Helps in data visualization and feature selection"
          ]
        }
      ],
      "activities": [
        {
          "type": "visualization",
          "title": "Clustering Visualization",
          "description": "Create and visualize clusters using different algorithms"
        },
        {
          "type": "project",
          "title": "Customer Segmentation",
          "description": "Apply clustering to segment customer data"
        }
      ]
    },
    {
      "lesson_id": "2.4",
      "title": "Data Preprocessing and Feature Engineering",
      "duration": "90 minutes",
      "topics": [
        {
          "topic_id": "2.4.1",
          "title": "Data Cleaning and Preprocessing",
          "content": "Data preprocessing is a crucial step in machine learning that involves cleaning, transforming, and preparing data for modeling. Quality data preprocessing significantly impacts model performance.",
          "key_points": [
            "Handle missing values through imputation or removal",
            "Detect and remove outliers that can skew results",
            "Normalize or standardize numerical features",
            "Encode categorical variables for algorithm compatibility"
          ]
        },
        {
          "topic_id": "2.4.2",
          "title": "Feature Engineering",
          "content": "Feature engineering is the process of creating new features from existing data to improve model performance. This is often where domain expertise and creativity come into play.",
          "key_points": [
            "Create new features from existing ones",
            "Extract meaningful information from raw data",
            "Combine features to capture interactions",
            "Feature engineering can significantly improve model performance"
          ]
        },
        {
          "topic_id": "2.4.3",
          "title": "Feature Selection and Scaling",
          "content": "Feature selection helps choose the most relevant features, while scaling ensures all features contribute equally to the model. Both techniques are essential for optimal model performance.",
          "key_points": [
            "Feature selection reduces dimensionality and overfitting",
            "Scaling ensures features are on the same scale",
            "Common scaling methods: StandardScaler, MinMaxScaler",
            "Feature importance helps in selection decisions"
          ]
        }
      ],
      "activities": [
        {
          "type": "hands_on",
          "title": "Data Preprocessing Pipeline",
          "description": "Build a complete data preprocessing pipeline"
        },
        {
          "type": "analysis",
          "title": "Feature Engineering Case Study",
          "description": "Analyze and create features for a real dataset"
        }
      ]
    },
    {
      "lesson_id": "2.5",
      "title": "Model Training and Evaluation",
      "duration": "120 minutes",
      "topics": [
        {
          "topic_id": "2.5.1",
          "title": "Training and Testing Split",
          "content": "Properly splitting data into training and testing sets is crucial for evaluating model performance. This prevents overfitting and provides realistic performance estimates.",
          "key_points": [
            "Split data into training (70-80%) and testing (20-30%) sets",
            "Use stratified sampling for classification problems",
            "Cross-validation provides more robust evaluation",
            "Never use test data for training or feature selection"
          ]
        },
        {
          "topic_id": "2.5.2",
          "title": "Model Evaluation Metrics",
          "content": "Different evaluation metrics are used depending on the problem type. Understanding these metrics is essential for interpreting model performance and making improvements.",
          "key_points": [
            "Classification metrics: Accuracy, Precision, Recall, F1-Score",
            "Regression metrics: MSE, MAE, R-squared",
            "ROC-AUC: Useful for binary classification",
            "Choose metrics based on business requirements"
          ]
        },
        {
          "topic_id": "2.5.3",
          "title": "Overfitting and Underfitting",
          "content": "Overfitting occurs when a model learns the training data too well but fails to generalize. Underfitting happens when the model is too simple. Balancing these is key to good model performance.",
          "key_points": [
            "Overfitting: Model performs well on training data but poorly on new data",
            "Underfitting: Model is too simple and performs poorly overall",
            "Regularization techniques help prevent overfitting",
            "Cross-validation helps detect overfitting"
          ]
        }
      ],
      "activities": [
        {
          "type": "experiment",
          "title": "Model Evaluation Comparison",
          "description": "Compare different models using various evaluation metrics"
        },
        {
          "type": "analysis",
          "title": "Overfitting Detection",
          "description": "Identify and address overfitting in model training"
        }
      ]
    },
    {
      "lesson_id": "2.6",
      "title": "Real-World Machine Learning Applications",
      "duration": "60 minutes",
      "topics": [
        {
          "topic_id": "2.6.1",
          "title": "Industry Applications",
          "content": "Machine learning is applied across various industries, from healthcare to finance to retail. Understanding these applications helps in choosing the right approaches for specific domains.",
          "key_points": [
            "Healthcare: Disease diagnosis, drug discovery, patient monitoring",
            "Finance: Fraud detection, risk assessment, algorithmic trading",
            "Retail: Recommendation systems, inventory management",
            "Transportation: Autonomous vehicles, route optimization"
          ]
        },
        {
          "topic_id": "2.6.2",
          "title": "Ethical Considerations in ML",
          "content": "Machine learning applications raise important ethical considerations including bias, privacy, transparency, and accountability. Understanding these issues is crucial for responsible ML development.",
          "key_points": [
            "Bias in data can lead to unfair predictions",
            "Privacy concerns with personal data usage",
            "Transparency and explainability are important",
            "Accountability for ML system decisions"
          ]
        },
        {
          "topic_id": "2.6.3",
          "title": "Best Practices and Deployment",
          "content": "Successfully deploying machine learning models requires careful planning, monitoring, and maintenance. Understanding deployment challenges helps in building robust ML systems.",
          "key_points": [
            "Model versioning and tracking is essential",
            "Continuous monitoring detects performance degradation",
            "A/B testing helps evaluate model improvements",
            "Documentation and reproducibility are crucial"
          ]
        }
      ],
      "activities": [
        {
          "type": "case_study",
          "title": "ML Application Analysis",
          "description": "Analyze a real-world ML application and its challenges"
        },
        {
          "type": "discussion",
          "title": "ML Ethics Discussion",
          "description": "Discuss ethical considerations in machine learning"
        }
      ]
    }
  ],
  "assessments": [
    {
      "assessment_id": "1",
      "type": "quiz",
      "title": "Machine Learning Fundamentals Quiz",
      "description": "Comprehensive quiz covering all module topics",
      "questions": [
        {
          "question": "What is the main difference between supervised and unsupervised learning?",
          "type": "multiple_choice",
          "options": [
            "Supervised learning is faster than unsupervised learning",
            "Supervised learning uses labeled data, unsupervised uses unlabeled data",
            "Unsupervised learning is more accurate than supervised learning",
            "Supervised learning only works with numerical data"
          ],
          "correct_answer": 1
        },
        {
          "question": "Which of the following is NOT a supervised learning algorithm?",
          "type": "multiple_choice",
          "options": [
            "Linear Regression",
            "K-means Clustering",
            "Random Forest",
            "Support Vector Machine"
          ],
          "correct_answer": 1
        },
        {
          "question": "What is the purpose of feature engineering?",
          "type": "multiple_choice",
          "options": [
            "To reduce the dataset size",
            "To create new features that improve model performance",
            "To make the code run faster",
            "To reduce the number of algorithms available"
          ],
          "correct_answer": 1
        },
        {
          "question": "What is overfitting in machine learning?",
          "type": "multiple_choice",
          "options": [
            "When a model is too simple and performs poorly",
            "When a model learns the training data too well but fails to generalize",
            "When the dataset is too large",
            "When there are too many features"
          ],
          "correct_answer": 1
        },
        {
          "question": "Which evaluation metric is most appropriate for binary classification?",
          "type": "multiple_choice",
          "options": [
            "Mean Squared Error",
            "R-squared",
            "Accuracy",
            "All of the above"
          ],
          "correct_answer": 2
        }
      ]
    },
    {
      "assessment_id": "2",
      "type": "project",
      "title": "Machine Learning Project",
      "description": "Complete a machine learning project from data preprocessing to model deployment",
      "requirements": [
        "Choose a dataset and define the problem",
        "Perform data preprocessing and feature engineering",
        "Train multiple models and compare performance",
        "Evaluate models using appropriate metrics",
        "Present findings and recommendations"
      ]
    }
  ],
  "resources": [
    {
      "type": "reading",
      "title": "Introduction to Machine Learning with Python",
      "author": "Andreas C. Müller and Sarah Guido",
      "description": "Comprehensive guide to machine learning with practical examples"
    },
    {
      "type": "video",
      "title": "Machine Learning Course",
      "source": "Stanford CS229",
      "url": "https://cs229.stanford.edu/"
    },
    {
      "type": "article",
      "title": "A Tour of Machine Learning Algorithms",
      "source": "Machine Learning Mastery",
      "description": "Overview of different machine learning algorithms and their applications"
    },
    {
      "type": "interactive",
      "title": "Machine Learning Playground",
      "source": "Google",
      "description": "Interactive visualization of machine learning concepts"
    }
  ],
  "next_module": "AI Foundation - Module Three: Deep Learning Fundamentals"
} 